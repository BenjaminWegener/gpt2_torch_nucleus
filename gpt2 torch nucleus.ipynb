{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civil-republic",
   "metadata": {},
   "source": [
    "# torch gpt2\n",
    "### from https://amaarora.github.io/2020/02/18/annotatedGPT2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "toxic-cycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python36\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: filelock in c:\\python36\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\python36\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\python36\\lib\\site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\python36\\lib\\site-packages (from transformers) (3.7.3)\n",
      "Requirement already satisfied: requests in c:\\python36\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging in c:\\python36\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python36\\lib\\site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in c:\\python36\\lib\\site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python36\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python36\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\python36\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: click in c:\\python36\\lib\\site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\python36\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\python36\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\python36\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python36\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python36\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\python36\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python36\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python36\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python36\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\python36\\lib\\site-packages (from click->sacremoses->transformers) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 21.1.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "Invalid requirement: '#for'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 93, in __init__\n",
      "    req = REQUIREMENT.parseString(requirement_string)\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1632, in parseString\n",
      "    raise exc\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1622, in parseString\n",
      "    loc, tokens = self._parse( instring, 0 )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1379, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3395, in parseImpl\n",
      "    loc, exprtokens = e._parse( instring, loc, doActions )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1379, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3717, in parseImpl\n",
      "    return self.expr._parse( instring, loc, doActions, callPreParse=False )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1379, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3378, in parseImpl\n",
      "    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1383, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 2689, in parseImpl\n",
      "    raise ParseException(instring, loc, self.errmsg, self)\n",
      "pip._vendor.pyparsing.ParseException: Expected W:(abcd...) (at char 0), (line:1, col:1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 252, in from_line\n",
      "    req = Requirement(req)\n",
      "  File \"c:\\python36\\lib\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 97, in __init__\n",
      "    requirement_string[e.loc:e.loc + 8]))\n",
      "pip._vendor.packaging.requirements.InvalidRequirement: Invalid requirement, parse error at \"'#for'\"\n",
      "\n",
      "You are using pip version 10.0.1, however version 21.1.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: entmax in c:\\python36\\lib\\site-packages (1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 21.1.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "!pip install transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "!pip install wget #for downloading the weights (500MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precise-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv1d layer/module\n",
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sealed-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed-forward module\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integrated-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    \n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        outputs = torch.matmul(scores, v)\n",
    "        return outputs\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(*new_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        out      = self._attn(q, k, v)\n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regular-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting it all together with transformer block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
    "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.feedforward(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all blocks put together\n",
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
    "        self.h       = _get_clones(block, 12)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(0.1)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, src, labels=None, pos_ids=None):\n",
    "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
    "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return outputs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "monetary-context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\python36\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 21.1.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under gpt2-pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (h): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build model and download weights from huggingface\n",
    "model = GPT2()\n",
    "# load pretrained_weights from hugging face\n",
    "# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.`\n",
    "if not(os.path.isfile('gpt2-pytorch_model.bin')):\n",
    "    !python -m wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n",
    "model_dict = model.state_dict() #currently with random initialization\n",
    "state_dict = torch.load(\"./gpt2-pytorch_model.bin\") #pretrained weights\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys(): \n",
    "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys): \n",
    "    state_dict[new_key]=state_dict.pop(old_key)\n",
    "\n",
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval() #model in inference mode as it's now initialized with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "revised-drain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 21.875s, prediction:\n",
      "The planet earth, then, the only way, it was the latter the former, then the former is the former, then the and now, that is it only and the only are to and to and then world the, and the it the and in. and to be that, in, it, is, and the, and to the world, and to the, and then to earth, then she in the, the earth to that planet, that, that to earth and that and be in to and\n"
     ]
    }
   ],
   "source": [
    "#run the model with the tokenizer from huggingface with input \"The planet earth \"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context   = torch.tensor([tokenizer.encode(\"The planet earth\")])\n",
    "\n",
    "def generate(context, ntok):\n",
    "    for _ in range(ntok):\n",
    "        out = model(context)\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "start = time.time()\n",
    "out = generate(context, ntok = 100)\n",
    "prediction = tokenizer.decode(out[0])\n",
    "end = time.time()\n",
    "print('time elapsed: ' + \"{:.3f}\".format(end - start) + 's, prediction:\\n' + prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sapphire-briefing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 23.397s, prediction:\n",
      "The planet earth has been completely changed. And have you seen the earth of the earth that and that. the. of the life that is really and has of. have whole, and planet of planet were, a. and in. planet. with. a- and, and. a. a a planet. a. earth and. and exist planet. the world by. plus has it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ", and\n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#use nucleus sampling from https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "        \n",
    "        Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "temperature = 0.7\n",
    "top_k = 0\n",
    "top_p = 0.9\n",
    "\n",
    "def generate(context, ntok):\n",
    "    for _ in range(ntok):\n",
    "        out = model(context)\n",
    "        logits = out[0, -1, :] / temperature\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        probabilities = F.softmax(filtered_logits, dim=-1)\n",
    "        next_tok = torch.multinomial(probabilities, 1)\n",
    "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "start = time.time()\n",
    "out = generate(context, ntok = 100)\n",
    "prediction = tokenizer.decode(out[0])\n",
    "end = time.time()\n",
    "print('time elapsed: ' + \"{:.3f}\".format(end - start) + 's, prediction:\\n' + prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-ceiling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-target",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
